# Kubernetes Observability Lab - Development Guidelines

## üéØ Project Context (Quick Reference)
- **LocalMart E-commerce Platform** - Learning environment for cloud-native observability
- **Current Phase**: 2.3.0 (Complete observability: Prometheus, Grafana, Loki, Tempo + Catalog service)
- **Target Audience**: DevOps engineers, SREs, developers learning production patterns
- **Approach**: Production patterns optimized for local learning (not production scale)

## üìö Documentation Roadmap

When I need detailed information, I should read these files:

| **Topic** | **Read This File** | **Specific Section** |
|-----------|-------------------|---------------------|
| **Project Overview** | `README.md` | Full introduction and architecture |
| **Service Implementation Patterns** | `services/catalog/README.md` | Complete reference implementation |
| **API Testing Examples** | `services/catalog/README.md` | `#testing-examples` section |
| **Observability Features** | `services/catalog/README.md` | `#observability-features-deep-dive` |
| **Code Organization** | `services/catalog/README.md` | `#code-organization-guide` |
| **Setup Instructions** | `README.md` | `#detailed-setup` section |
| **Architecture Diagrams** | `README.md` | `#architecture` section |
| **Traffic Simulation** | `scripts/README.md` | Traffic generation patterns |
| **Documentation Sync** | `scripts/WORKFLOW.md` | End-of-session doc update workflow |

## ‚ö†Ô∏è Critical Safety Rules

**ALWAYS follow these rules:**
- ‚úÖ Use `./kubectl-lab` (never raw `kubectl`) - protects against wrong context
- ‚úÖ Use `./tilt-lab` (never raw `tilt`) - preconfigured with correct kubeconfig  
- ‚úÖ Verify dependency versions with official docs (`go get`, `helm repo add`)
- ‚úÖ Test Kubernetes manifests in isolation before adding to Tiltfile
- ‚úÖ All configurations are LOCAL ONLY - never apply to production clusters
- ‚úÖ Run `./scripts/sync-docs.sh` at end of sessions to keep docs synchronized

## ü§ñ AI Collaboration Guidelines

**What works well:**
- Documentation and comprehensive code reviews
- Boilerplate generation (K8s manifests, Go handlers)
- Architecture analysis and pattern suggestions
- Debugging complex error logs (especially Kubernetes events)

**What needs human verification:**
- Dependency management and version selection
- OpenTelemetry endpoint configurations  
- Service discovery and networking configs
- Integration between observability components

**End-of-session workflow:**
- Run `./scripts/sync-docs.sh analyze --since-tag` to identify what docs need updates
- Update only the specific sections mentioned (token-efficient)
- Validate with `./scripts/sync-docs.sh validate`

## üõ†Ô∏è Development Decision Tree

**User asks about...**
- **"How do I implement X in a service?"** ‚Üí Read `services/catalog/` as reference pattern
- **"How do I test the API?"** ‚Üí Point to `services/catalog/README.md#testing-examples`
- **"How does tracing/logging/metrics work?"** ‚Üí Reference `services/catalog/README.md#observability`
- **"How do I add a new service?"** ‚Üí Follow catalog service pattern, update Tiltfile + ingress
- **"How do I see traces/logs?"** ‚Üí Direct to Grafana examples in catalog service docs
- **"What's the architecture?"** ‚Üí Reference README.md diagrams
- **Setup issues** ‚Üí Point to README.md setup section

## üìÅ Quick File Reference

**Core files to know:**
- `services/catalog/` - Complete reference implementation
- `k8s/observability/` - Observability stack configurations  
- `k8s/apps/catalog/` - Catalog service K8s manifests
- `Tiltfile` - Automated deployment configuration
- `scripts/simulate-traffic.sh` - Generate observability data

**Common commands:**
- `./start-lab.sh` - Initialize environment
- `./tilt-lab up` - Deploy everything
- `./kubectl-lab get pods -A` - Check all services
- `./scripts/simulate-traffic.sh` - Generate test data

## üéØ Key Patterns to Follow

**When adding new services:**
1. Follow `services/catalog/` structure and patterns
2. Include observability from day one (tracing, logging, metrics)
3. Use clean architecture (handlers ‚Üí models ‚Üí database)
4. Implement comprehensive testing examples
5. Create service-specific README with API docs

**Response format consistency:**
- API responses: `{"data": {...}, "page": 1, "count": 25}` 
- Health endpoints: `{"data": {"status": "healthy", "database": "connected"}}`
- Error responses: Include request correlation

---

**For comprehensive information, always read the actual documentation files listed above rather than trying to recall everything from this context.**